trained.data.directory=data/wsd/data
stemmer.hashmap.dir=data/wsd/Stems

# no need to configure anything below of UnlEnco


#Wed Oct 10 01:27:39 IST 2007
#This parameter is set to mention the number of folds for n-fold cross validation
no.of.folds = 1
#Activate N-Fold Splitter
activate.folds = false

# Corpus Info. These 3 parameters mention the base directory of the corpus.
# If n-fold validation is being used then the n-fold splitting will be done
# and the directories Train-1 to Tran-n and Test-1 to Test-n will be created
# inside these directories.
# Please note that all the 3 parameters have the same value. This is because
# two of the 3 variables are redundant (corpus.base.dir and train.base.dir) but are being used for backward compatibility.
corpus.base.dir=../Data/eng/Tagged/TOURISM_training
train.base.dir=../Data/eng/Tagged/TOURISM_training
test.base.dir=not_in_jar/test_files

enable.output=true
output.dir=not_in_jar/output
# Results will get stored here.
results.base.dir=not_in_jar/Results

# Mentions the domain of the coprus. Used for storing the statistics learnt while training in 
# the appropriate directory (namely ../resources/SupWSDResources/<domain>). 
corpus.domain=TOURISM
#exact location where trained word and sense statistics are stored



# Both these parameters indicate the language of the corpus. 
# While running SupervisedWSD both the parameters should have the same value which 
# is the language of the corpus.
# While ParallelDictionaryGenerator to learn statistics for MAR from sense tagged Hindi
# corpus set source.language=HIN and target.language=MAR.
source.language=ENG
target.language=ENG
pivot.language=HIN

# Used to perform inverse n fold validation i.e. test on 25% and train on 75% -- Whats this?
inverse.n.fold=false

# This field sets the minimum threshold for considering P(S|word). (Keep it 30 always)
min.support=0
# This context window using which WSD should be performed. Setting it to 5 means it will 
# consider 5 words on left and 5 words on right.
context.window = 5

#orderingMethod:0=PolysemyBasedOrder;1=EntropyBasedOrder
order.method=1
#ordertype:0=Increasing;1=Decreasing
order.type=0
lambda.weight = 0.5

# Thresholds for entering a word in WN++
# If no. of instances less than this and P(S|word) > confidence then add word to WN++
wn.plus.plus.lower.threshold = 5
# If no. of instances greater than this and P(S|word) > confidence then add word to WN++
wn.plus.plus.upper.threshold = 30
# If P(S|word) > confidence and one of above two conditions is satisfied then add word to WN++
wn.plus.plus.confidence = 0.9

#If the algorithm is not able to disambiguate a word then select default sense for the word.
default.as.baseline=true




# The statistics learnt after training (by running SupervisedWSD) are stored in ../resources/SupWSDResources/<domain>
# So no need to train again and again.
do.training = false
# Set this to false if you are interested only in trainig.
do.testing = true
# Set this to false if you do not want to run the baseline algorithm.
check.baseline = false

#ITERATIVE_WSD_0 = 0/media/ZIPPY/workspace/Data/eng/Tagged;
#ITERATIVE_WSD_N = 1;
#WN_PLUS_PLUS = 2;
#WN_PLUS_PLUS_ITERATIVE_WSD = 3;
#MOST_FREQUENT_SENSE = 4;
#PAGE_RANK = 5;
#enable.strategies=0,1,2,3,4,5
#The strategies listed here will be run and results will be reported. 
enable.strategies=1

#0=All senses have minscore; 1=one sense has 70% prob; 2=Top 2 senses have only 0.1 difference in probability;3=Nothing
compare.vi=3
# For transferring features from one language to another.
# Mention the parallel corpus directories. In addition set source.language and 
# target.language appropriately.

source.test.base.dir=../Data/mar/Tagged/HEALTH
target.test.base.dir=../Data/mar/Tagged/HEALTH
# for language Independence checker
source.trained.base.dir=../Data/hin/Statistics
target.trained.base.dir=../Data/mar/Statistics
#enable this flag to populate the missing wordStatistics to improve recall
all.wordStatistics=true


#For debugging
test.description= For WN++ thresholds are 30, 5 and 0.9. For iterative WSD min.support=30
anomaly.files =  
#anomaly.files = Tr_0100_eng_tagged.txt,Tr_0103_eng_tagged.txt,Tr_0104_eng_tagged.txt,Tr_0105_eng_tagged.txt,Tr_0109_eng_tagged.txt,Tr_0110_eng_tagged.txt
feature.train.base.dir=../resources/ParallelCorpusAligned/Tourism/hin/Tagged/Test-1
sense.stats.base.dir=Stats/Tourism/Hindi
feature.stats.base.dir=FeatureStatsBOW/Tourism/Hindi
feature.context.window = 30
#test.base.dir=../resources/ParallelCorpus/hin/Common/Test-4
#fold.train.base.dir=../resources/ParallelCorpus/hin/Common/Train-
#fold.test.base.dir=../resources/ParallelCorpus/hin/Common/Test-


CDInfoHM.base.dir = ../resources/CDInfo/
CDInfoHM.conceptLevel = 0
# value 0 indicates that CD is not used in Disambiguation
# Currently for Hindi and English, Information upto level 3 is precomputed.

#Initialze stemmer for all other languages. 

init.stemmer.language = TAM,BEN,PAN

##############################
senseCombination.maxCount = -1

#########################saurabh
projection.strategy = 0
#0 = no projection at all, only use true langaue parameter statistics
#1 = always do projection irrespective of whether true parameters present or not
#2 = do parameter projection if true parameters are absent (best suitable for ILILMT)
#3 = mix true (source language) and projected (Hindi-Pivot) parameters statistics with true.parameter.weitht 

# defines the weight beta with which true parameter statistics are to be mixed with projected statistics
true.parameter.weitht = 5

#Making The Engine Language Independent
Pivot.WordStatisticsDir=not_in_jar/SupWSDResources/TOURISM/HIN
Pivot.SenseStatisticsDir=not_in_jar/SupWSDResources/TOURISM/HIN

crosslinks.directory = not_in_jar/SupWSDResources/Crosslinks
use.crosslinks = true
